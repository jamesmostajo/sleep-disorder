{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('stroke_data.csv').dropna()\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['stroke'])\n",
    "y = df[['stroke']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(X_train.values, dtype=torch.float32), torch.tensor(y_train.values, dtype=torch.float32))\n",
    "test_ds = TensorDataset(torch.tensor(X_test.values, dtype=torch.float32), torch.tensor(y_test.values, dtype=torch.float32))\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=20, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TunableModel(nn.Module):\n",
    "    def __init__(self, num_hidden_nodes, num_hidden_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # add Input Layer\n",
    "        self.input_layer = nn.Linear(10, num_hidden_nodes)\n",
    "\n",
    "        # Add a Mechanism that would add n number of hidden layers\n",
    "        hidden_layers = []\n",
    "        for i in range(num_hidden_layers):\n",
    "            hidden_layers.append(nn.Linear(num_hidden_nodes, num_hidden_nodes))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "        \n",
    "        # Add a Sequential Portion to house all hidden layers\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "\n",
    "        # Add Output Layer\n",
    "        self.output_layer = nn.Linear(num_hidden_nodes, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        pred = self.sigmoid(x)\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "# Training Steps\n",
    "def train(model, dataloader, epochs, loss_fn, optimizer, verbose=True):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        i = 0\n",
    "        for X_val, y_val in dataloader:\n",
    "            pred = model(X_val)\n",
    "            loss = loss_fn(pred, y_val)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            # Print Tracker only every 100 steps\n",
    "            if i % 100 == 0 and verbose:\n",
    "                print('Epoch {0} Step {1}: Loss - {2}'.format(epoch+1, i, loss.item()), end='\\r')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    # Make Predictions\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in dataloader:\n",
    "            pred = model(X_val).round()\n",
    "            preds.extend(pred.numpy())\n",
    "            trues.extend(y_val.numpy())\n",
    "    \n",
    "    return f1_score(trues, preds)\n",
    "\n",
    "\n",
    "# Define our optimization loop\n",
    "\n",
    "def score_hyperparameter_sets(hyperparameter_sets):\n",
    "    scores = []\n",
    "\n",
    "    # Iterate through the entire set to get scores\n",
    "    for hyperparameter_set in hyperparameter_sets:\n",
    "        # Set Random State for Reproducibility\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        # Unpack all hyperparameters in the set\n",
    "        hidden_nodes = hyperparameter_set['hidden_nodes']\n",
    "        hidden_layers = hyperparameter_set['hidden_layers']\n",
    "        # Define the Model with hyperparameters\n",
    "        model = TunableModel(num_hidden_nodes=hidden_nodes, num_hidden_layers=hidden_layers)\n",
    "\n",
    "        # Define Training Objects\n",
    "        loss_fn = nn.BCELoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train Model\n",
    "        print(f'Training Model with HP: (Hidden Nodes-{hidden_nodes}, Hidden Layers-{hidden_layers})')\n",
    "        model = train(model, train_dl, 10, loss_fn, optimizer)\n",
    "\n",
    "        # Test Model\n",
    "        score = test(model, test_dl)\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "\n",
    "    # Make Table to show the results\n",
    "    results = pd.DataFrame(hyperparameter_sets)\n",
    "    results['score'] = scores\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_space = {\n",
    "    'hidden_nodes': range(20, 200+1, 20),\n",
    "    'hidden_layers': range(0, 5, 1)\n",
    "}\n",
    "hyperparameter_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hp_space_to_sets(hpo_space):\n",
    "    hp_sets = []\n",
    "    for hidden_nodes in hpo_space['hidden_nodes']:\n",
    "        for hidden_layers in hpo_space['hidden_layers']:\n",
    "            hp_sets.append({'hidden_nodes': hidden_nodes, 'hidden_layers': hidden_layers})\n",
    "    \n",
    "    return hp_sets\n",
    "\n",
    "hp_sets = convert_hp_space_to_sets(hyperparameter_space)\n",
    "hp_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = score_hyperparameter_sets(hp_sets)\n",
    "results_df.sort_values(by='score', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
